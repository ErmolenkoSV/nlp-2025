{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subword tokenization\n",
    "\n",
    "**Проблема:**\n",
    "\n",
    "* Для каждого языка - свой морфологический анализатор. Проблема для синтетических языков.\n",
    "* Новые слова, имена, опечатки → **OOV (out-of-vocabulary)**.\n",
    "* Частотное распределение слов — **длинный хвост** (закон Ципфа).\n",
    "* Для нейронный сетей нужно ограничить размер словаря.\n",
    "\n",
    "**Решение:**\n",
    "\n",
    "* Разделить слова на **подслова (subwords)**.\n",
    "* Получить компактный словарь и возможность собирать редкие слова.\n",
    "\n",
    "--\n",
    "\n",
    "## Пример\n",
    "\n",
    "| Слово         | Частота | Обычная токенизация | Subword токенизация       |\n",
    "| ------------- | ------- | ------------------- | ------------------------- |\n",
    "| *голодный*    | высокая | `[\"голод\"]`         | `[\"голод\", \"ный\"]`        |\n",
    "| *несчастный*  | низкая  | `[\"несчастн\"]`      | `[\"не\", \"счаст\", \"ный\"]`  |\n",
    "| *мирззрение*  | редкая  |   (OOV)             | `[\"мир\", \"з\", \"зрение\"]`  |\n",
    "\n",
    "\n",
    "\n",
    "Можно сжать словарь, улучшить обобщение, сохранить смысловую структуру.\n",
    "\n",
    "---\n",
    "\n",
    "## Основная идея\n",
    "\n",
    "Токенизация превращает текст в последовательность **единиц (токенов)**, которые:\n",
    "\n",
    "* часто встречаются,\n",
    "* могут комбинироваться для построения новых слов.\n",
    "\n",
    "\n",
    "| Уровень     | Пример                 | Проблемы                           |\n",
    "| ----------- | ---------------------- | ---------------------------------- |\n",
    "| Символы     | `[\"p\", \"l\", \"a\", \"y\"]` | слишком длинные последовательности |\n",
    "| Слова       | `[\"play\", \"games\"]`    | OOV, огромный словарь              |\n",
    "| Части слов  | `[\"play\", \"ing\"]`      | компромисс                         |\n",
    "\n",
    "---\n",
    "\n",
    "## Алгоритмы Subword Tokenization\n",
    "\n",
    "### Byte Pair Encoding (BPE)\n",
    "\n",
    "Идея:\n",
    "\n",
    "* Начинаем с отдельных символов.\n",
    "* Итеративно **сливаем самые частые пары** в единый токен.\n",
    "\n",
    "Пример:\n",
    "\n",
    "```\n",
    "t h e r e -> t + h + e + r + e\n",
    "```\n",
    "\n",
    "сливаем частые пары: `t+h`, `th+e`, `the+r` и т.д.\n",
    "\n",
    "Постепенно получаем: `the`, `there`, и т.д.\n",
    "\n",
    "BPE строит словарь от символов до подсловных единиц.\n",
    "\n",
    "---\n",
    "\n",
    "### Пример BPE шагов\n",
    "\n",
    "| Шаг | Частые пары | Новые токены |\n",
    "| --- | ----------- | ------------ |\n",
    "| 1   | `t`+`h`     | `th`         |\n",
    "| 2   | `th`+`e`    | `the`        |\n",
    "| 3   | `e`+`r`     | `er`         |\n",
    "| 4   | `the`+`r`   | `ther`       |\n",
    "\n",
    "---\n",
    "\n",
    "### WordPiece\n",
    "\n",
    "* Используется в **BERT**, **DistilBERT**, **ALBERT**.\n",
    "* Похож на BPE, но критерий — **максимизация вероятности** слова по языковой модели, а не частоты пар.\n",
    "\n",
    "Формально:\n",
    "$$\n",
    "\\text{argmax}_V \\sum_{w \\in D} \\log P(w | V)\n",
    "$$\n",
    "\n",
    "Преимущества:\n",
    "\n",
    "* Более статистически обоснованный.\n",
    "* Лучше работает на данных с разной морфологией.\n",
    "\n",
    "---\n",
    "\n",
    "### Unigram Language Model (SentencePiece)\n",
    "\n",
    "* Используется в **SentencePiece**\n",
    "* Начинает с **большого набора возможных субслов**, затем **удаляет** наименее вероятные.\n",
    "* Основан на вероятностной модели:\n",
    "  $$\n",
    "  P(w) = \\prod_i P(s_i)\n",
    "  $$\n",
    "  где $s_i$ — подслова.\n",
    "\n",
    "Преимущества:\n",
    "\n",
    "* Позволяет хранить несколько возможных разбиений.\n",
    "* Гибче, чем BPE.\n",
    "\n",
    "---\n",
    "\n",
    "### Byte-level BPE (GPT-2, GPT-3)\n",
    "\n",
    "* Работает на уровне **байтов**, а не символов Unicode.\n",
    "* Универсален: поддерживает любой язык, даже эмодзи.\n",
    "* Не требует нормализации.\n",
    "\n",
    "Пример:\n",
    "\n",
    "```\n",
    "Text: \"Привет!\" → bytes → merges → subwords\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Сравнение алгоритмов\n",
    "\n",
    "| Алгоритм           | Основа                     | Преимущества       | \n",
    "| ------------------ | -------------------------- | ------------------ | \n",
    "| **BPE**            | Частотность пар            | Прост, быстрый     | \n",
    "| **WordPiece**      | Максимизация вероятности   | Более точен        | \n",
    "| **Unigram**        | Модель вероятности подслов | Гибкий, устойчивый | \n",
    "| **Byte-level BPE** | Байты                     | Универсален        | \n",
    "\n",
    "---\n",
    "\n",
    "## Популярные библиотеки\n",
    "\n",
    "| Библиотека                  | Описание                            | Алгоритмы               |\n",
    "| --------------------------- | ----------------------------------- | ----------------------- |\n",
    "| **SentencePiece** (Google)  | Самая гибкая                        | Unigram, BPE            |\n",
    "| **Hugging Face Tokenizers** | Быстрая реализация на Rust          | BPE, WordPiece, Unigram |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Пример BPE \n",
    "\n",
    "Текст:\n",
    "\n",
    "```\n",
    "low lower lowest\n",
    "```\n",
    "\n",
    "Начало:\n",
    "\n",
    "```\n",
    "l o w _ l o w e r _ l o w e s t\n",
    "```\n",
    "\n",
    "1. Самая частая пара: `l o` → `lo`\n",
    "2. Затем `lo w` → `low`\n",
    "3. Потом `low e` → `lowe` и т.д.\n",
    "\n",
    "Результат:\n",
    "\n",
    "```\n",
    "[\"low\", \"er\", \"est\"]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Использование Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from typing import Iterable\n",
    "\n",
    "# Чтение файла данных\n",
    "def read_texts(fn: str=\"data/news.txt.gz\") -> Iterable[str]:\n",
    "    with gzip.open(fn, \"rt\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            yield line.strip().split(\"\\t\")[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents\n",
    "from tokenizers.decoders import WordPiece as WordPieceDecoder\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "\n",
    "s = \"hello_world\"\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "tokenizer.normalizer = normalizer\n",
    "tokenizer.decoder = WordPieceDecoder()\n",
    "\n",
    "trainer = WordPieceTrainer(vocab_size=8000)\n",
    "\n",
    "tokenizer.train_from_iterator(read_texts(), trainer=trainer)\n",
    "tokenizer.save(\"data/news_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"data/news_tokenizer.json\")\n",
    "res = tokenizer.encode(\"первому корове привет миру прививка\")\n",
    "\n",
    "\n",
    "res.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "\n",
    "tokenizer.enable_padding(length=50)\n",
    "tokenizer.encode(\"первому корове привет миру прививка\")\n",
    "\n",
    "\n",
    "res = tokenizer.encode_batch([\"первому корове привет миру прививка\", \"хорошая погода\"])\n",
    "torch.tensor([x.ids for x in res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = nn.Embedding(10000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"hello world\" -> [5, 7, 0, 0, 0] -> [[0.5, 0.2, ...], [0.7, 0.9, ....]]\n",
    "res = tokenizer.encode_batch([\"первому корове привет миру прививка при\", \"хорошая погода\"])\n",
    "input = torch.tensor([x.ids for x in res])\n",
    "f = emb(input)\n",
    "f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_nlp (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
