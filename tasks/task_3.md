# Применение инструментов Hugging face и предобученных моделей

## Вариант 1:
Необходимо создать искусственные данные для тестирования чат-бота. По заданному короткому предложению (команде или утверждению) требуется сгенерировать несколько (5–10) семантически близких формулировок, сохраняющих исходный смысл, но отличающихся лексикой и синтаксисом.

Разрешается использовать только общедоступные предобученные модели размером до ~7B параметров (Hugging Face).

Необходимо:

1. Реализовать генерацию парафразов.
2. Предложить и реализовать механизм автоматической валидации качества (например, через embedding similarity, NLI, classifier и т.п.).
3. Будет плюсом попробовать обучение / дообучение / prompt-tuning.

Пример:

> After your workout, remember to focus on maintaining a good water balance.

похожие команды:

> Remember to drink enough water to restore and maintain your body's hydration after your cardio training.

> Please don't forget to maintain water balance after workout.


## Вариант 2:

Нужно реализовать простейшую семантическую **поисковую систему** c помощью векторного представления предложений/текстов.
1. Выбрать коллекцию текстовых документов (небольшое подмножество статей из Википедии (из дампа), новости, и т.п.).
2. Выбрать модель для получения векторных представлений (например [sentence-transformers](https://huggingface.co/sentence-transformers)).
3. Выбрать векторное хранилище (faiss, lancedb, qdrant, chroma, pgvector, redis и т.д.)
4. Реализовать поиск, (возможно с постфильтрацией) и продемонстрировать его работу. Индексация и поиск должны быть реализованы в виде отдельных скриптов с CLI.

Нельзя использовать LangChain и подобные готовые системы. 

## Вариант 3:

Реализовать Named Entity Recognition (NER) с помощью предобученных небольших локальных LLM (<= 7B). Подобрать промпты для структурной генерации. 

Сравнить и проанализировать различия в качестве с предобученными моделями на основе (на выбор):
1. CRF (sklearn-crfsuite)
2. spaCy (pretrained NER)
3. BERT-like (например hf: dslim/bert-base-NER)
4. Natasha
5. ...

Язык и тестовый набор данных выбираются самостоятельно. Наличие дообучения будет рассматриваться как преимущество. Дистилляция модели также будет дополнительным плюсом.

