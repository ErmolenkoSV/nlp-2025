{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bacdeb16",
   "metadata": {},
   "source": [
    "# BERT (Encoder-Only) для NLP приложений\n",
    "\n",
    "## Введение в BERT\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers, 2018) — это предобученная модель на основе трансформеров, использующая только encoder-часть архитектуры.\n",
    "\n",
    "### Ключевые особенности:\n",
    "- НЕ генеративная модель \n",
    "- Предобучена на задачах Masked Language Modeling (MLM) и Next Sentence Prediction (NSP)\n",
    "- Эффективна для различных задач NLP\n",
    "- Генерирует контекстно-зависимые эмбеддинги\n",
    "- Варианты: ModernBERT (2024/25), RoBERTa, DistilBERT, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837dc8ec",
   "metadata": {},
   "source": [
    "## Классификация предложений\n",
    "\n",
    "### Задача: Sentiment Analysis\n",
    "\n",
    "Определение тональности отзывов (положительный/отрицательный)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef7524d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: 46.00%\n",
      "Negative: 54.00%\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "text = \"This product is amazing! I love it.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# ToDo: !!!добавить обучение!!!\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    outputs = model(**inputs)\n",
    "    predictions = F.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "print(f\"Positive: {predictions[0][1]:.2%}\")\n",
    "print(f\"Negative: {predictions[0][0]:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2f1d34",
   "metadata": {},
   "source": [
    "## Классификация токенов\n",
    "\n",
    "### Задача: Извлечение именованных сущностей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e0bee86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El: B-ORG (score: 0.91)\n",
      "##on: I-ORG (score: 0.84)\n",
      "Mu: I-ORG (score: 0.69)\n",
      "##sk: I-ORG (score: 0.73)\n",
      "Te: B-ORG (score: 1.00)\n",
      "##sla: I-ORG (score: 0.99)\n",
      "California: B-LOC (score: 1.00)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# Создание пайплайна для NER\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\", model=\"dslim/bert-base-NER\", tokenizer=\"dslim/bert-base-NER\"\n",
    ")\n",
    "\n",
    "# Анализ текста\n",
    "text = \"Elon Musk founded Tesla in California.\"\n",
    "ner_results = ner_pipeline(text)\n",
    "\n",
    "# Вывод результатов\n",
    "for entity in ner_results:\n",
    "    print(f\"{entity['word']}: {entity['entity']} (score: {entity['score']:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16affc15",
   "metadata": {},
   "source": [
    "## Question Answering\n",
    "\n",
    "### Задача: Извлечение ответов из текста"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2f2d7f",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "# Загрузка модели\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "# Контекст и вопрос\n",
    "context = \"BERT was published in 2018 by researchers at Google AI Language.\"\n",
    "question = \"When was BERT published?\"\n",
    "\n",
    "# Токенизация\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "\n",
    "# Получение ответа\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    answer_start = torch.argmax(outputs.start_logits)\n",
    "    answer_end = torch.argmax(outputs.end_logits) + 1\n",
    "    \n",
    "answer = tokenizer.convert_tokens_to_string(\n",
    "    tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end])\n",
    ")\n",
    "\n",
    "print(f\"Ответ: {answer}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735dfc8c",
   "metadata": {},
   "source": [
    "## Emdeddings\n",
    "\n",
    "### Вычисление sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce2923eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Схожесть (sent1, sent2): 0.8712\n",
      "Схожесть (sent1, sent3): 0.6416\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "def mean_pooling(\n",
    "    model_output: torch.Tensor, attention_mask: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    token_embeddings = model_output.last_hidden_state  # [B, L, H]\n",
    "\n",
    "    # Приводим mask к размерности [B, L, 1]\n",
    "    input_mask_expanded = (\n",
    "        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    )\n",
    "\n",
    "    # Суммируем только токены, где mask == 1\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)\n",
    "\n",
    "    # Делим на количество реальных токенов\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "\n",
    "def get_sentence_embedding(text: str) -> torch.Tensor:\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    embedding = mean_pooling(outputs, inputs[\"attention_mask\"])\n",
    "\n",
    "    return embedding.squeeze(0)\n",
    "\n",
    "\n",
    "# Два предложения для сравнения\n",
    "sent1 = \"The cat sits on the mat.\"\n",
    "sent2 = \"A cat is sitting on a rug.\"\n",
    "sent3 = \"The weather is sunny today.\"\n",
    "\n",
    "# Получение эмбеддингов\n",
    "emb1 = get_sentence_embedding(sent1)\n",
    "emb2 = get_sentence_embedding(sent2)\n",
    "emb3 = get_sentence_embedding(sent3)\n",
    "\n",
    "# Вычисление косинусной схожести\n",
    "similarity_1_2 = F.cosine_similarity(emb1, emb2, dim=0).item()\n",
    "similarity_1_3 = F.cosine_similarity(emb1, emb3, dim=0).item()\n",
    "\n",
    "print(f\"Схожесть (sent1, sent2): {similarity_1_2:.4f}\")\n",
    "print(f\"Схожесть (sent1, sent3): {similarity_1_3:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4f17d8",
   "metadata": {},
   "source": [
    "## Заполнение пропусков (Masked Language Modeling)\n",
    "\n",
    "### Задача: Предсказание замаскированных слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f496be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "france: 0.9280\n",
      "brittany: 0.0084\n",
      "algeria: 0.0074\n",
      "department: 0.0050\n",
      "reunion: 0.0044\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Создание пайплайна\n",
    "fill_mask = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "\n",
    "# Предсказание замаскированного слова\n",
    "text = \"The capital of [MASK] is Paris.\"\n",
    "predictions = fill_mask(text)\n",
    "\n",
    "# Вывод топ-5 предсказаний\n",
    "for pred in predictions[:5]:\n",
    "    print(f\"{pred['token_str']}: {pred['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7681fe71",
   "metadata": {},
   "source": [
    "## Fine-tuning на кастомных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9057f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/alex/evo840/work/envs/venv_nlp/lib/python3.11/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Подготовка данных\n",
    "texts = [\"Great!\", \"Terrible\", \"Not bad\", \"Excellent quality\"]\n",
    "labels = [1, 0, 1, 1]  # 1 - positive, 0 - negative\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_dataset = CustomDataset(texts, labels, tokenizer)\n",
    "\n",
    "# Загрузка модели\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Создание Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Обучение\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b70118",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del fill_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5f0366",
   "metadata": {},
   "source": [
    "# HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92277dd3",
   "metadata": {},
   "source": [
    "Использование pipeline's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b942e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT2: My name is John [?]\n",
    "# BERT: My name is John.<sep> Playstation. \n",
    "#       My [?] is John [?]. \n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8353f1b",
   "metadata": {},
   "source": [
    "обработка одного предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "553047ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 7592, 2088, 25120, 102]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"hello world meaningless\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679c28e8",
   "metadata": {},
   "source": [
    "идентификаторы в токены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "803d0bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[SEP]']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "209704be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] hello world meaningless [SEP]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(\n",
    "    token_ids=[101, 7592, 2088, 25120, 102],\n",
    "    skip_special_tokens=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3937443b",
   "metadata": {},
   "source": [
    "сразу несколько предложений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96ee9b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  7592,  2088, 25120,   102],\n",
       "        [  101, 14380,  3392,   102,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 0]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\n",
    "    text=[\"hello world meaningless\", \"lemon tree\"],\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18daae63",
   "metadata": {},
   "source": [
    "Вывод \"вручную\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0be9ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=None, logits=tensor([[[ -7.1909,  -7.0791,  -7.0640,  ...,  -6.1842,  -6.2977,  -4.2038],\n",
       "         [ -9.6314,  -9.4228,  -9.5366,  ...,  -9.2171,  -8.3938,  -5.9639],\n",
       "         [-12.5930, -12.9342, -12.8723,  ..., -11.7490, -11.0491,  -4.2497],\n",
       "         [-12.2999, -12.2242, -12.0449,  ..., -10.0910,  -9.9382, -11.6595]]],\n",
       "       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "inp = tokenizer(\"hello world\", return_tensors=\"pt\")\n",
    "\n",
    "result = model(**inp)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eee143e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([12.3461, 10.5820, 10.4628, 10.1078,  9.7246], grad_fn=<TopkBackward0>),\n",
       "indices=tensor([ 3000, 22479, 10241, 16766,  7562]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "sent = f\"The capital of France is {tokenizer.mask_token}.\"\n",
    "inp = tokenizer(sent, return_tensors=\"pt\")\n",
    "\n",
    "logits = model(**inp).logits\n",
    "\n",
    "\n",
    "torch.topk(logits[0, 6, :], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f8472d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paris']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a075b51",
   "metadata": {},
   "source": [
    "С помощью pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c654098c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.01388245727866888,\n",
       "  'token': 4202,\n",
       "  'token_str': 'taylor',\n",
       "  'sequence': 'my name is sasha taylor!'},\n",
       " {'score': 0.011390585452318192,\n",
       "  'token': 5954,\n",
       "  'token_str': 'stewart',\n",
       "  'sequence': 'my name is sasha stewart!'},\n",
       " {'score': 0.010675176978111267,\n",
       "  'token': 3656,\n",
       "  'token_str': 'alexander',\n",
       "  'sequence': 'my name is sasha alexander!'},\n",
       " {'score': 0.010375653393566608,\n",
       "  'token': 2100,\n",
       "  'token_str': '##y',\n",
       "  'sequence': 'my name is sashay!'},\n",
       " {'score': 0.007814432494342327,\n",
       "  'token': 12214,\n",
       "  'token_str': 'winters',\n",
       "  'sequence': 'my name is sasha winters!'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"fill-mask\", \"bert-base-uncased\")\n",
    "\n",
    "pipe(\"my name is Sasha [MASK]!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a7807c",
   "metadata": {},
   "source": [
    "\"Casual\" LM. Генерация текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6b7106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "030ae112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"What is the largest ocean in the world? It's not clear. We know that the ocean is a finite resource, but that doesn't mean that it can't be used to fuel life.\\n\\nThis article was reprinted with permission from the Center for Science in the Public Interest.\"}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokens = tokenizer(\"hello world\", return_tensors=\"pt\")\n",
    "#model(**tokens)\n",
    "\n",
    "pipe = pipeline(\"text-generation\", \"gpt2\")\n",
    "pipe(\"What is the largest ocean in the world?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b5f190",
   "metadata": {},
   "source": [
    "## Sentence embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "421f3c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87b19620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0999, -2.4728, 35.9304]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = model.encode(\"Paris is the capital of France\")\n",
    "ans = model.encode([\"hello world\", \"play game\", \"Париж является столицей Франции\"])\n",
    "\n",
    "util.dot_score(q, ans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_nlp (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
