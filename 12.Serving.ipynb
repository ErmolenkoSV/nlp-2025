{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e0d223e",
   "metadata": {},
   "source": [
    "# vLLM, llama.cpp и SGLang\n",
    "\n",
    "## Зачем  нужны движки инференса?\n",
    "\n",
    "* Обучение LLM — **дорого и редко**\n",
    "* Инференс — **часто, массово и в продакшене**\n",
    "* Наивный PyTorch-инференс:\n",
    "\n",
    "  * медленный\n",
    "  * плохо масштабируется\n",
    "  * неэффективно использует GPU / CPU\n",
    "\n",
    "**Движки инференса решают:**\n",
    "\n",
    "* высокую пропускную способность (throughput)\n",
    "* низкую задержку (latency)\n",
    "* экономию памяти\n",
    "\n",
    "\n",
    "## Общая картина\n",
    "\n",
    "| Движок        | Где используется           |\n",
    "| ------------- | -------------------------- |\n",
    "| **vLLM**      | Серверы, API, облака       |\n",
    "| **llama.cpp** | Локальные машины, embedded |\n",
    "| **SGLang**    | Агентные системы, RAG      |\n",
    "\n",
    "\n",
    "# vLLM\n",
    "\n",
    "**vLLM** — это высокопроизводительный движок инференса LLM на GPU.\n",
    "\n",
    "Основная идея: **максимально эффективно использовать GPU-память и батчинг запросов**\n",
    "\n",
    "OpenAI-compatible API\n",
    "\n",
    "\n",
    "Важная идея vLLM — PagedAttention\n",
    "\n",
    "Проблема:\n",
    "\n",
    "* KV-cache занимает много памяти\n",
    "* разные запросы имеют разную длину\n",
    "\n",
    "Решение:\n",
    "\n",
    "* KV-cache хранится как **страницы (pages)**\n",
    "* динамическое выделение и переиспользование\n",
    "\n",
    "Плюсы:\n",
    "\n",
    "* меньше fragmentation\n",
    "* больше одновременных запросов\n",
    "\n",
    "\n",
    "## Пример использования vLLM\n",
    "\n",
    "```python\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "llm = LLM(model=\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "params = SamplingParams(temperature=0.8, max_tokens=128)\n",
    "\n",
    "outputs = llm.generate([\"Explain transformers\"], params)\n",
    "print(outputs[0].outputs[0].text)\n",
    "```\n",
    "\n",
    "- Нужно обслуживать **много пользователей**\n",
    "- Есть **GPU**\n",
    "- Важен throughput\n",
    "\n",
    "\n",
    "# llama.cpp\n",
    "\n",
    "\n",
    "**llama.cpp** — C++ реализация инференса LLaMA-подобных моделей.\n",
    "\n",
    "Цели:\n",
    "\n",
    "* минимальные зависимости\n",
    "* работа на CPU\n",
    "* переносимость\n",
    "\n",
    "Запускается:\n",
    "\n",
    "* на ноутбуке\n",
    "* на Raspberry Pi\n",
    "* на сервере без GPU\n",
    "\n",
    "Проблема:\n",
    "\n",
    "* FP16/FP32 слишком большие\n",
    "\n",
    "Решение:\n",
    "\n",
    "* INT8 / INT4 / INT2\n",
    "* форматы GGML / GGUF\n",
    "\n",
    "Пример:\n",
    "\n",
    "* 7B модель\n",
    "\n",
    "  * FP16 ≈ 14 GB\n",
    "  * Q4 ≈ 4 GB\n",
    "\n",
    "\n",
    "## Архитектура llama.cpp\n",
    "\n",
    "* Чистый C/C++\n",
    "* SIMD (AVX, NEON)\n",
    "* OpenMP\n",
    "* Metal / CUDA (опционально)\n",
    "\n",
    "Фокус:\n",
    "\n",
    "* latency одного запроса\n",
    "* энергоэффективность\n",
    "\n",
    "\n",
    "## Пример запуска llama.cpp\n",
    "\n",
    "```bash\n",
    "./llama-cli \\\n",
    "  -m llama-7b-q4.gguf \\\n",
    "  -p \"Explain backpropagation\"\n",
    "```\n",
    "\n",
    "Или через Python bindings:\n",
    "\n",
    "```python\n",
    "from llama_cpp import Llama\n",
    "llm = Llama(model_path=\"model.gguf\")\n",
    "llm(\"Hello\")\n",
    "```\n",
    "\n",
    "* Локальный запуск\n",
    "* Нет GPU\n",
    "\n",
    "\n",
    "# SGLang\n",
    "\n",
    "\n",
    "**SGLang** — runtime для управления LLM-инференсом.\n",
    "\n",
    "* control-flow\n",
    "* программирование диалогов\n",
    "* агентные сценарии\n",
    "\n",
    "\n",
    "Наивный код:\n",
    "\n",
    "* много prompt engineering\n",
    "* сложная логика в Python\n",
    "\n",
    "SGLang:\n",
    "\n",
    "* декларативные шаблоны\n",
    "* разделение логики и инференса\n",
    "\n",
    "\n",
    "## Пример SGLang\n",
    "\n",
    "```python\n",
    "@sgl.function\n",
    "def qa(ctx, question):\n",
    "    ctx += \"Answer the question:\"\n",
    "    ctx += question\n",
    "    ctx += sgl.gen(\"answer\", max_tokens=100)\n",
    "    return ctx[\"answer\"]\n",
    "```\n",
    "\n",
    "## Архитектура SGLang\n",
    "\n",
    "* Frontend: DSL / Python API\n",
    "* Runtime: scheduler\n",
    "* Backend:\n",
    "\n",
    "  * vLLM\n",
    "  * TensorRT-LLM\n",
    "\n",
    "SGLang **использует** vLLM, но не заменяет его\n",
    "\n",
    "## Где используется SGLang\n",
    "\n",
    "* RAG pipelines\n",
    "* multi-step reasoning\n",
    "* tool calling\n",
    "* агенты\n",
    "\n",
    "Особенно полезен когда:\n",
    "\n",
    "* сложная логика генерации\n",
    "* много взаимодействий с LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207b9b5e",
   "metadata": {},
   "source": [
    "## Пример с `llama.cpp` и `OpenAI API`\n",
    "\n",
    "Запуск:\n",
    "\n",
    "`docker run -p 8080:8080 ghcr.io/ggml-org/llama.cpp:server -hf ggml-org/gemma-3-270m-it-GGUF --port 8080 --host 0.0.0.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4365163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, I'm ready to explain the concept of RAG (Retrieval-Augmented Generation). Let's break it down!\n",
      "\n",
      "**What is RAG?**\n",
      "\n",
      "RAG is a technique that combines the strengths of traditional search engines (like Google) with the power of generative AI. It aims to provide more relevant and accurate information to users in the future, especially for tasks that require factual knowledge.\n",
      "\n",
      "Think of it like this:\n",
      "\n",
      "*   **Traditional Search:** You type a keyword into a search box. The search engine looks for relevant results based on your keywords.\n",
      "*   **RAG:** You type a keyword into a search box and then ask the search engine to retrieve relevant information from a vast database of websites and information.\n",
      "\n",
      "**Key Components of RAG:**\n",
      "\n",
      "1.  **Knowledge Base:** This is the core of the system. It's a collection of websites, articles, and other resources that the system can access to answer questions and provide answers. This could be a library of books, a website with a comprehensive FAQ, or even a database of industry-specific knowledge.\n",
      "\n",
      "2.  **Retrieval Mechanism:** This is the process of getting the information from the knowledge base. It involves:\n",
      "    *   **Querying the Knowledge Base:** You use a search query to find relevant information.\n",
      "    *   **Filtering and Ranking:** The search engine uses filters to narrow down the results based on relevance, accuracy, and other factors. It then ranks the results based on these factors.\n",
      "    *   **Summarization:** The retrieved information is often summarized to provide a concise overview.\n",
      "\n",
      "3.  **Generation:** This is the process of using the retrieved information to generate a new response. It involves:\n",
      "    *   **Content Creation:** The system creates new content based on the retrieved information. This could be a new article, a blog post, a product description, or a question-answer pair.\n",
      "    *   **Fine-tuning:** The system is often fine-tuned on a dataset of questions and answers to improve its accuracy and relevance. This involves training the system on a large amount of data to understand the nuances of different types of questions and answers.\n",
      "\n",
      "**Benefits of RAG:**\n",
      "\n",
      "*   **Improved Accuracy:** By providing more accurate and up-to-date information, RAG helps users find the right answers quickly and easily.\n",
      "*   **Reduced Search Time:** By providing relevant information, RAG reduces the time users spend searching for answers.\n",
      "*   **Better User Experience:** RAG can make the search process more intuitive and user-friendly.\n",
      "*   **Increased User Engagement:** By providing more accurate and up-to-date information, RAG can encourage users to engage with the search engine more frequently.\n",
      "*   **Reduced Need for Human Expertise:** RAG can help reduce the need for human experts to answer questions, as the information is provided by the system.\n",
      "\n",
      "**How RAG Works in Practice:**\n",
      "\n",
      "1.  **User Input:** You provide a question or query to the search engine.\n",
      "2.  **Search Engine Analysis:** The search engine analyzes your query and identifies relevant information from its knowledge base.\n",
      "3.  **Information Retrieval:** The search engine retrieves the relevant information from the knowledge base.\n",
      "4.  **Information Synthesis:** The system combines the retrieved information with the user's question or query to generate a new answer.\n",
      "5.  **Response Generation:** The system creates a new response based on the retrieved information.\n",
      "6.  **User Interaction:** You interact with the search engine again to request more information or to ask further questions.\n",
      "\n",
      "**Examples of RAG Applications:**\n",
      "\n",
      "*   **Question Answering:**  If you ask a question, the search engine can answer it by retrieving relevant information from its knowledge base.\n",
      "*   **Summarization:**  If you search for a topic, the search engine can summarize the information into a concise overview.\n",
      "*   **Content Creation:**  If you search for a topic, the system can create a new article or blog post.\n",
      "*   **Knowledge Management:**  RAG can help organizations store and manage their knowledge by providing access to relevant information from a vast database.\n",
      "*   **Customer Support:**  RAG can help support customers by providing answers to their questions and resolving their issues quickly.\n",
      "\n",
      "**Challenges of RAG:**\n",
      "\n",
      "*   **Accuracy:**  The accuracy of RAG depends on the quality and completeness of the knowledge base.\n",
      "*   **Scalability:**  Scaling RAG systems to handle a large volume of queries can be challenging.\n",
      "*   **Maintenance:**  RAG systems require ongoing maintenance to ensure that they remain accurate and up-to-date.\n",
      "*   **Cost:**  Implementing and maintaining RAG systems can be expensive.\n",
      "\n",
      "**In conclusion, RAG is a powerful technique that leverages the power of AI to provide more relevant and accurate information to users in the future. By combining the strengths of traditional search engines with the power of generative AI, RAG can improve the user experience, reduce search time, and increase user engagement.**\n",
      "\n",
      "Do you have any specific questions about RAG that you'd like me to answer? Let me know!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    base_url=\"http://localhost:8080/v1\", \n",
    "    api_key=\"sk-no-key-required\" \n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"local-model\", \n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain the concept of RAG\"}\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_nlp (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
