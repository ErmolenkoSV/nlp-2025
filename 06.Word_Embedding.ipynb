{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Non Contextual Word embedding\n",
    "\n",
    "## Мотивация\n",
    "\n",
    "* Машинное обучение требует **числового представления** данных.\n",
    "* Но **слова — это символы**, не числа.\n",
    "* Нужен способ преобразовать их в векторы, чтобы:\n",
    "\n",
    "  * сравнивать слова,\n",
    "  * находить похожие,\n",
    "  * использовать в моделях NLP.\n",
    "\n",
    "---\n",
    "\n",
    "## One-Hot Encoding / TF-IDF\n",
    "\n",
    "* Простейший способ кодирования слов:\n",
    "\n",
    "  * Каждое слово — вектор длиной `|V|` (размер словаря)\n",
    "  * Только одна позиция равна 1, остальные — 0\n",
    "\n",
    "Пример:\n",
    "\n",
    "```python\n",
    "V = [\"cat\", \"dog\", \"fish\"]\n",
    "cat = [1, 0, 0]\n",
    "dog = [0, 1, 0]\n",
    "fish = [0, 0, 1]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Проблемы One-Hot Encoding\n",
    "\n",
    "* **Разреженность:** вектор огромный, почти весь из нулей\n",
    "* **Отсутствие семантики:** \"cat\" и \"dog\" не ближе друг к другу, чем \"cat\" и \"car\"\n",
    "* **Невозможность обобщения:** новые слова не связаны с уже известными\n",
    "\n",
    "---\n",
    "\n",
    "## Идея Word Embedding\n",
    "\n",
    "* Представим каждое слово как **плотный вектор** небольшой размерности (например, 100–1000)\n",
    "* Цель: слова с **похожим смыслом → близкие векторы**\n",
    "\n",
    "Пример (векторы 2D для иллюстрации):\n",
    "\n",
    "| слово | вектор       | смысл           |\n",
    "| ----- | ------------ | --------------- |\n",
    "| king  | (0.8, 0.2)   | власть          |\n",
    "| queen | (0.79, 0.25) | власть + женск. |\n",
    "| man   | (0.4, 0.1)   | мужской         |\n",
    "| woman | (0.42, 0.13) | женский         |\n",
    "\n",
    "---\n",
    "\n",
    "## Семантические отношения в embedding-пространстве\n",
    "\n",
    "Векторы отражают смысловые отношения:\n",
    "\n",
    "```\n",
    "king - man + woman ≈ queen\n",
    "```\n",
    "\n",
    "→ Векторы можно складывать, вычитать, искать аналоги.\n",
    "\n",
    "---\n",
    "\n",
    "## Подходы к построению embedding'ов\n",
    "\n",
    "1. **Модели на основе контекста (Word2Vec, FastText)**\n",
    "   — слово -> context\n",
    "2. **Модели на основе статистики (GloVe)**\n",
    "   — используют глобальную статистику совместных встреч\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "\n",
    "\n",
    "[Word2vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) - один из первых методов векторного представления слов,  ставших популярным. \n",
    "\n",
    "> \"Смысл слова определяется контекстом, в котором оно встречается.\"\n",
    "\n",
    "(дистрибутивная гипотеза)\n",
    "\n",
    "Два основных варианта:\n",
    "\n",
    "1. **CBOW (Continuous Bag of Words)**\n",
    "\n",
    "   * Предсказывает текущее слово по контексту\n",
    "   * Пример: `the white cat ___ on the mat` → `sit`\n",
    "\n",
    "2. **Skip-gram**\n",
    "\n",
    "   * Предсказывает контекст по текущему слову\n",
    "   * Пример: `cat` → (`the`, `white`, `sat`, `on`, `mat`)\n",
    "\n",
    "\n",
    "![Источник - https://arxiv.org/pdf/1301.3781.pdf ](img/w2v.png)\n",
    "\n",
    "\n",
    "Вероятность слова в контексте можно интерпретировать как:\n",
    "\n",
    "$$p(w_o|w_I) = \\frac{\\exp(v′_{w_O}^T \\cdot v_{w_I})}{\\sum_{w=1}^{W}\\exp(v′_{w_i}^T \\cdot v_{w_I})}$$\n",
    "\n",
    "Для оптимизации используется или negative sampling или hierarchical softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText\n",
    "\n",
    "[fastText](https://fasttext.cc/) - библиотека от Facebook.\n",
    "- большое количество предтренированныйх моделей для разных языков\n",
    "- может сопоставлять вектора для слов вне словаря\n",
    "\n",
    "Принцип работы - слово делится на N-граммы. Каждой N-грамме сопоставляется вектор, для получения векторного представления всего слова вектора N-грамм суммируются. \n",
    "\n",
    "**привет** $\\rightarrow$ **&lt;пр**, **при**, **рив**, **иве**, **вет**, **ет&gt;**. \n",
    "\n",
    "* Учитывает **части слова**, а не только целое\n",
    "* Понимает **редкие и новые слова**\n",
    "\n",
    "* Например, \"cats\" → похоже на \"cat\"\n",
    "* Особенно полезен для **морфологически богатых языков** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "привет -> [, , ,]\n",
    "\n",
    "hello -> [, , , ,]\n",
    "\n",
    "лес -> [, , , ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe\n",
    "\n",
    "[GloVe](https://nlp.stanford.edu/pubs/glove.pdf) - имплементация через минимизацию функционала:\n",
    "\n",
    "$$J = \\sum_{i,j=1}^V f(X_{ij}) (w_i \\cdot \\tilde{w_j} + b_i + \\tilde{b_j} - \\log X_{ij})$$\n",
    "\n",
    "где, $X_{ij}$ - матрица взаимовстречаемости слов (сколько раз слово $i$ имело в контексте слова $j$). \n",
    "\n",
    "$f(x)= \\begin{cases}\n",
    "    (\\frac{x}{x_{max}})^\\alpha, & \\text{если }  x<x_{max}.\\\\\n",
    "    1, & \\text{иначе}.\n",
    "  \\end{cases}\n",
    "$\n",
    "\n",
    "* Использует **всю статистику корпуса**, а не только локальные окна\n",
    "* Быстрее обучается на больших текстах\n",
    "* Хорошо отражает аналогии и семантические связи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ограничения классических embeddings\n",
    "\n",
    "* Один вектор на слово → не различает **омонимы**\n",
    "  (например, *bank* — «берег» vs «банк»)\n",
    "* Не учитывает **контекст**\n",
    "* Для этого появились **контекстные эмбеддинги** (ELMo, BERT, GPT)\n",
    "\n",
    "---\n",
    "\n",
    "| Этап      | Пример                    | Особенности              |\n",
    "| --------- | ------------------------- | ------------------------ |\n",
    "| 2013–2016 | Word2Vec, GloVe, FastText | статические              |\n",
    "| 2018+     | ELMo, BERT, GPT           | контекстные              |\n",
    "| 2020+     | LLMs                      | универсальные embeddings |\n",
    "\n",
    "\n",
    "\n",
    "* Анализ тональности (sentiment analysis)\n",
    "* Рекомендательные системы\n",
    "* Поиск по смыслу (semantic search)\n",
    "* Классификация текстов\n",
    "* Кластеризация документов\n",
    "\n",
    "## Визуализация embeddings\n",
    "\n",
    "* Используют **t-SNE** или **UMAP** для понижения размерности\n",
    "* Можно увидеть кластеры:\n",
    "\n",
    "  * география (St. Petersburg, Berlin, Rome)\n",
    "  * глаголы (run, walk, swim)\n",
    "  * персоналии "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример обучения Word2Vec с помощью [gensim](https://github.com/RaRe-Technologies/gensim)\n",
    "\n",
    "\n",
    "Каждая строчка в архиве - тройка\n",
    "> **category** \\\\t **headline** \\\\t **text**\n",
    "\n",
    "Где:\n",
    "- category - категоря новости\n",
    "- headline - заголовок\n",
    "- text - текст новости\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterator, List\n",
    "\n",
    "@dataclass\n",
    "class Text:\n",
    "    label: str\n",
    "    title: str\n",
    "    text: str\n",
    "\n",
    "# Чтение файла данных\n",
    "def read_texts(fn: str=\"data/news.txt.gz\") -> Iterator[Text]:\n",
    "    with gzip.open(fn, \"rt\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            yield Text(*line.strip().split(\"\\t\"))\n",
    "                    \n",
    "# Разбиение текста на слова                 \n",
    "def tokenize_text(text: str) -> List[str]:\n",
    "    text = text.lower()\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return words\n",
    "\n",
    "# Текст без знаков припенания (нужен для gensim)\n",
    "def normalize_text(text: str) -> str:\n",
    "    return ' '.join(tokenize_text(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучение word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('р', 0.9689614176750183),\n",
       " ('советский', 0.9140341281890869),\n",
       " ('итар', 0.8822527527809143),\n",
       " ('экспресс', 0.8762419819831848),\n",
       " ('marca', 0.8718820214271545),\n",
       " ('известия', 0.8707618117332458),\n",
       " ('lifenews', 0.8662394881248474),\n",
       " ('рбк', 0.855144202709198),\n",
       " ('коммерсантъ', 0.8542253375053406),\n",
       " ('интерфакс', 0.8522573113441467)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Обучение word2vec\n",
    "# каждый текст - набор слов через пробел\n",
    "sentences = [tokenize_text(text.text) for text in read_texts()]\n",
    "\n",
    "# обучаем w2v\n",
    "w2v = Word2Vec(sentences)\n",
    "\n",
    "# сохраняем модель\n",
    "w2v.wv.save_word2vec_format('w2v_vectors.bin')\n",
    "# пример\n",
    "w2v.wv.most_similar(\"новости\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_nlp (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
